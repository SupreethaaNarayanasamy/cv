# -*- coding: utf-8 -*-
"""IPCV_Endsem.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NLJAFV5Pc4Fah_JburSHVaq5u8u1K3kr
"""

#EX - 1.1 (pixel wise difference)
!pip install opencv-python

import cv2
import matplotlib.pyplot as plt

img1=cv2.imread("image1.jpg")
img2=cv2.imread("image2.jpg")
diff_img="output.jpeg"

if img1 is None:
    raise FileNotFoundError(f"Could not open Image1")
if img2 is None:
    raise FileNotFoundError(f"Could not open Image2")

if img1.shape!=img2.shape:
  h,w=img1.shape[:2]
  img2=cv2.resize(img2,(w,h),interpolation=cv2.INTER_AREA)

diff=cv2.absdiff(img1,img2)
diff_gray=cv2.cvtColor(diff,cv2.COLOR_BGR2GRAY)

def show(title,img):
  if len(img.shape)==3:
    img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
  plt.figure()
  plt.title(title)
  plt.imshow(img,cmap="gray" if len(img.shape)==2 else None)
  plt.axis("off")
show("Image1",img1)
show("Image2",img2)
show("Output",diff)

print("Press any key to exit")
cv2.waitKey(0)
cv2.destroyAllWindows()

cv2.imwrite(diff_img,diff)

#EX 1.2 (feature extraction using HOG)
!pip install scikit-image

import cv2
from skimage.feature import hog
import numpy as np
import matplotlib.pyplot as plt

img=cv2.imread("image1.jpg")
img=cv2.resize(img,(64,128))
gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)


#gradient calculation
gx=cv2.Sobel(gray,cv2.CV_64F,1,0,ksize=1)
gy=cv2.Sobel(gray,cv2.CV_64F,0,1,ksize=1)

#magnitude and orientation
mag=np.sqrt(gx**2+gy**2)
orientation=(np.arctan2(gy,gx)*180/np.pi)%180

#figures
plt.figure(figsize=(10,4))
plt.subplot(1,4,1)
plt.title("x gradient")
plt.imshow(gx,cmap="gray")
plt.axis("off")

plt.subplot(1,4,2)
plt.title("y gradient")
plt.imshow(gy,cmap="gray")
plt.axis("off")

plt.subplot(1,4,3)
plt.title("Maginitude")
plt.imshow(mag,cmap="gray")
plt.axis("off")

plt.subplot(1,4,4)
plt.title("Orientation")
plt.imshow(orientation,cmap="gray")
plt.axis("off")

#hog
features,hog_image=hog(
    gray,
    orientations=9,
    pixels_per_cell=(8,8),
    cells_per_block=(2,2),
    block_norm="L2-Hys",
    visualize="True"
)

plt.figure()
plt.title("Hog features")
plt.imshow(hog_image,cmap="gray")
plt.axis("off")


print("Number of features extracted:",len(features))

#Ex2.1 (contrast stretching and linear filtering and plot its histogram)

import cv2
import numpy as np
import matplotlib.pyplot as plt

img=cv2.imread("rose.jpg")
gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)

#contrast stretching
min_img=np.min(gray)
max_img=np.max(gray)
stretched_img=((img-min_img)/(max_img-min_img)*255).astype(np.uint8)

#filtering
filtered_img=cv2.GaussianBlur(stretched_img,(5,5),0)

#figures
plt.figure(figsize=(10,4))
plt.subplot(1,3,1)
plt.title("Original")
plt.imshow(gray,cmap="gray")
plt.axis("off")

plt.subplot(1,3,2)
plt.title("Contrast Stretched")
plt.imshow(stretched_img,cmap="gray")
plt.axis("off")

plt.subplot(1,3,3)
plt.title("Filtered")
plt.imshow(filtered_img,cmap="gray")
plt.axis("off")

#Histogram of filtered image
plt.figure()
plt.title("Histogram of filtered")
plt.hist(filtered_img.ravel(),bins=256)
plt.show()

#EX 2.2 ( Horse and Human dataset)
# Install required packages
!pip install tensorflow tensorflow-datasets matplotlib

import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
import matplotlib.pyplot as plt

# a) Load Horse vs Human dataset from TensorFlow Datasets
data, info = tfds.load('horses_or_humans', with_info=True, as_supervised=True)
train_ds = data['train'].take(800)
val_ds = data['train'].skip(800)

# b) View number of images
print("Training images:", tf.data.experimental.cardinality(train_ds).numpy())
print("Validation images:", tf.data.experimental.cardinality(val_ds).numpy())

# Normalize pixel values (rescale=1./255)
def normalize_img(image, label):
    image = tf.image.resize(image, (150, 150))
    image = tf.cast(image, tf.float32) / 255.0
    return image, label

train_ds = train_ds.map(normalize_img).batch(32).prefetch(tf.data.AUTOTUNE)
val_ds = val_ds.map(normalize_img).batch(32).prefetch(tf.data.AUTOTUNE)

# c) Plot some sample images
plt.figure(figsize=(8, 8))
for images, labels in train_ds.take(1):
    for i in range(9):
        plt.subplot(3, 3, i + 1)
        plt.imshow(images[i])
        plt.title("Horse" if labels[i] == 0 else "Human")
        plt.axis('off')
plt.show()

# e) Build CNN using Pretrained ResNet50
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(150, 150, 3))
base_model.trainable = False  # Freeze pretrained layers

model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(128, activation='relu'),
    Dense(1, activation='sigmoid')  # Binary output: Horse/Human
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.summary()

# f) Train the model
history = model.fit(train_ds, validation_data=val_ds, epochs=5)

# Plot training and validation accuracy
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training vs Validation Accuracy')
plt.legend()
plt.show()

'''Ex 3.1(geometrical transformations (scaling, rotation and shearing))'''
import cv2
import numpy as np
import matplotlib.pyplot as plt

# Load image
img = cv2.imread('image1.jpg')
rows, cols = img.shape[:2]

# 1) Scaling
scaled = cv2.resize(img, None, fx=1.5, fy=1.5)  # 150% size

# 2) Rotation
M_rot = cv2.getRotationMatrix2D((cols//2, rows//2), 45, 1)  # 45° rotation
rotated = cv2.warpAffine(img, M_rot, (cols, rows))

# 3) Shearing
M_shear = np.float32([[1, 0.5, 0], [0.5, 1, 0]])  # simple shear
sheared = cv2.warpAffine(img, M_shear, (int(cols*1.5), int(rows*1.5)))

# Show all transformations
plt.figure(figsize=(12,4))
plt.subplot(1,4,1)
plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
plt.title('Original'); plt.axis('off')

plt.subplot(1,4,2)
plt.imshow(cv2.cvtColor(scaled, cv2.COLOR_BGR2RGB))
plt.title('Scaled'); plt.axis('off')

plt.subplot(1,4,3)
plt.imshow(cv2.cvtColor(rotated, cv2.COLOR_BGR2RGB))
plt.title('Rotated'); plt.axis('off')

plt.subplot(1,4,4)
plt.imshow(cv2.cvtColor(sheared, cv2.COLOR_BGR2RGB))
plt.title('Sheared'); plt.axis('off')

plt.show()

'''Ex-3.2( feature extraction techniques using SIFT )'''
import cv2
import matplotlib.pyplot as plt

# Load grayscale image
img = cv2.imread('image1.jpg', cv2.IMREAD_GRAYSCALE)

# Step 1,2,3,4: SIFT
sift = cv2.SIFT_create()
keypoints, descriptors = sift.detectAndCompute(img, None)

# Draw keypoints with orientation
img_sift = cv2.drawKeypoints(
    img, keypoints, None,
    flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS
)

# Show image with keypoints
plt.figure(figsize=(8,6))
plt.imshow(img_sift, cmap='gray')
plt.title(f"SIFT Keypoints")
plt.axis('off')
plt.show()

# Print first 5 keypoints and their angles
print("First 5 keypoints (x, y, angle):")
for kp in keypoints[:5]:
    print(f"({kp.pt[0]:.1f}, {kp.pt[1]:.1f}), angle={kp.angle:.1f}°")

print("Number of keypoints detected:", len(keypoints))
print("Descriptor shape:", descriptors.shape)

'''Ex-4(Extract images from the video)'''
import cv2
import os

# 1. Define your specific output folder name here
output_folder = 'my_captured_frames'

# 2. Create the folder if it doesn't exist
os.makedirs(output_folder, exist_ok=True)

# 3. Load the video
cap = cv2.VideoCapture('/content/test.mov')
count = 0

while True:
    ret, frame = cap.read()
    if not ret:
        break

    # 4. Construct the full path correctly
    # This joins the folder name with the file name (e.g., 'my_captured_frames/frame_0.jpeg')
    file_name = f'frame_{count}.jpeg'
    full_path = os.path.join(output_folder, file_name)

    cv2.imwrite(full_path, frame)
    count += 1

cap.release()
print(f'Extracted {count} frames to the folder: "{output_folder}"')

'''Ex-5(MS-COCO)'''
import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt

# a) Load dataset
splits = ['validation[:80%]', 'validation[80%:]']
data, info = tfds.load('coco/2017', with_info=True, as_supervised=False, split=splits, try_gcs=True)

train_ds = data[0]
val_ds = data[1]

# b) Show number of images
print("Training images:", info.splits['train'].num_examples)
print("Validation images:", info.splits['validation'].num_examples)

# c) Plot sample images
plt.figure(figsize=(8, 8))
for i, sample in enumerate(train_ds.take(6)):
    image = sample['image']
    plt.subplot(2, 3, i+1)
    plt.imshow(image)
    plt.axis("off")
plt.show()

# f) Normalize images
def preprocess(image):
    image = tf.image.resize(image, (128, 128))
    image = tf.cast(image, tf.float32) / 255.0
    return image

train_images = train_ds.map(lambda x: preprocess(x['image'])).batch(32)
val_images = val_ds.map(lambda x: preprocess(x['image'])).batch(32)

# g) Build CNN model
model = models.Sequential([
    layers.Conv2D(32, (3,3), activation='relu', input_shape=(128,128,3)),
    layers.MaxPooling2D(2,2),
    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D(2,2),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')  # Dummy 10-class output
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# h) Train and evaluate
history = model.fit(train_images, validation_data=val_images, epochs=2)

# d) Augmentation
data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal_and_vertical"),
    layers.RandomRotation(0.2),
    layers.RandomContrast(0.3)
])

aug_train_ds = train_ds.map(lambda x: (data_augmentation(x['image']),))
aug_train_images = aug_train_ds.map(lambda x: preprocess(x[0])).batch(32)

print("After augmentation:")
print("Training images (approx):", info.splits['train'].num_examples)
print("Validation images:", info.splits['validation'].num_examples)

aug_model = models.clone_model(model)
aug_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

aug_history = aug_model.fit(aug_train_images, validation_data=val_images, epochs=2)

plt.plot(history.history['accuracy'], label='Before Aug')
plt.plot(aug_history.history['accuracy'], label='After Aug')
plt.title('Training Accuracy Comparison')
plt.legend()
plt.show()

#EX 5 (Workaround) MS-COCO
import tensorflow as tf
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt
import os
import requests
from concurrent.futures import ThreadPoolExecutor

# --- STEP 1: Manual "Mini COCO" Downloader ---
# Since TFDS crashes Colab, we manually fetch 50 specific images from COCO servers.

data_dir = 'coco_mini_dataset/train/class_0'
os.makedirs(data_dir, exist_ok=True)

# A list of valid COCO Validation image IDs
# (Random selection from val2017)
image_ids = [
    139, 285, 632, 724, 776, 785, 802, 872, 885, 1000,
    1268, 1296, 1353, 1425, 1490, 1503, 1532, 1584, 1675, 1761,
    1818, 1993, 2006, 2148, 2153, 2261, 2299, 2333, 2465, 2473,
    2532, 2685, 2923, 2929, 2953, 3155, 3236, 3255, 3357, 3457,
    3546, 3553, 3661, 3723, 3835, 3845, 3934, 3979, 4134, 4283
]

def download_image(img_id):
    # Formats ID to 12 digits (e.g., 139 -> 000000000139.jpg)
    file_name = f"{img_id:012d}.jpg"
    url = f"http://images.cocodataset.org/val2017/{file_name}"
    save_path = os.path.join(data_dir, file_name)

    if not os.path.exists(save_path):
        try:
            response = requests.get(url, timeout=5)
            if response.status_code == 200:
                with open(save_path, 'wb') as f:
                    f.write(response.content)
        except:
            pass # Skip if download fails

print(f"Downloading {len(image_ids)} COCO images directly...")
# Download in parallel to be fast
with ThreadPoolExecutor(max_workers=10) as executor:
    executor.map(download_image, image_ids)

print("Download complete. Files in folder:", len(os.listdir(data_dir)))

# --- STEP 2: Load Data using image_dataset_from_directory ---
# This replaces tfds.load

batch_size = 8
img_height = 128
img_width = 128

# Load Training Data (We use the same images for train/val for this tiny demo)
train_ds = tf.keras.utils.image_dataset_from_directory(
    'coco_mini_dataset',
    validation_split=0.2,
    subset="training",
    seed=123,
    image_size=(img_height, img_width),
    batch_size=batch_size,
    label_mode='int' # Uses folder names as labels (we only have class_0)
)

val_ds = tf.keras.utils.image_dataset_from_directory(
    'coco_mini_dataset',
    validation_split=0.2,
    subset="validation",
    seed=123,
    image_size=(img_height, img_width),
    batch_size=batch_size,
    label_mode='int'
)

# c) Plot sample images
plt.figure(figsize=(8, 4))
for images, labels in train_ds.take(1):
    for i in range(min(6, len(images))):
        plt.subplot(2, 3, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.axis("off")
plt.show()

# f) Preprocessing (Normalization)
normalization_layer = layers.Rescaling(1./255)
train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y)).cache().prefetch(tf.data.AUTOTUNE)
val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y)).cache().prefetch(tf.data.AUTOTUNE)

# g) Build CNN model
# Note: Since we only have 1 folder ('class_0'), the label is always 0.
# We output 1 class with sigmoid or just keep 10 and ignore the rest for the exercise sake.
model = models.Sequential([
    layers.Input(shape=(128, 128, 3)),
    layers.Conv2D(32, (3,3), activation='relu'),
    layers.MaxPooling2D(2,2),
    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D(2,2),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(1, activation='sigmoid') # Binary classification (Is it an image? Yes)
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# h) Train
print("\n--- Training Model ---")
history = model.fit(train_ds, validation_data=val_ds, epochs=5)

# d) Augmentation
data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal_and_vertical"),
    layers.RandomRotation(0.2),
])

# Create augmented dataset
# We need to apply augmentation BEFORE normalization effectively or inside the map
def augment_func(x, y):
    return data_augmentation(x), y

# Reload raw dataset for augmentation path to be clear
raw_train_ds = tf.keras.utils.image_dataset_from_directory(
    'coco_mini_dataset', validation_split=0.2, subset="training", seed=123,
    image_size=(img_height, img_width), batch_size=batch_size, label_mode='int'
)
aug_train_ds = raw_train_ds.map(augment_func).map(lambda x, y: (normalization_layer(x), y))

print("\n--- Training Augmented Model ---")
aug_model = models.clone_model(model)
aug_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
aug_history = aug_model.fit(aug_train_ds, validation_data=val_ds, epochs=5)

# Compare
plt.plot(history.history['accuracy'], label='Original')
plt.plot(aug_history.history['accuracy'], label='Augmented')
plt.legend()
plt.show()

#EX6
import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt

# a) Load BCCD dataset
data, info = tfds.load('bccd', with_info=True, as_supervised=False)
train_ds = data['train']
val_ds = data['validation']
test_ds = data['test']

# b) Show number of images
print("Train:", info.splits['train'].num_examples)
print("Validation:", info.splits['validation'].num_examples)
print("Test:", info.splits['test'].num_examples)


plt.figure(figsize=(8,8))
for i, sample in enumerate(train_ds.take(6)):
    img = sample['image']
    label = sample['objects']['label'][0]  # first object’s label
    plt.subplot(2,3,i+1)
    plt.imshow(img)
    plt.title(f"Label: {info.features['objects'].feature['label'].int2str(label)}")
    plt.axis('off')
plt.show()


def preprocess(sample):
    img = sample['image']
    # If multiple objects, take the first label (just for classification demo)
    label = sample['objects']['label'][0]
    img = tf.image.resize(img, (128, 128))
    img = tf.cast(img, tf.float32) / 255.0
    return img, label

train_ds_proc = train_ds.map(preprocess).batch(32).prefetch(tf.data.AUTOTUNE)
val_ds_proc = val_ds.map(preprocess).batch(32).prefetch(tf.data.AUTOTUNE)
test_ds_proc = test_ds.map(preprocess).batch(32).prefetch(tf.data.AUTOTUNE)


model = models.Sequential([
    layers.Conv2D(32, (3,3), activation='relu', input_shape=(128,128,3)),
    layers.MaxPooling2D(2,2),
    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D(2,2),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(info.features['objects'].feature['label'].num_classes, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(train_ds_proc, validation_data=val_ds_proc, epochs=5)
# Evaluate on test
test_loss, test_acc = model.evaluate(test_ds_proc)
print("Test accuracy before augmentation:", test_acc)


data_aug = tf.keras.Sequential([
    layers.RandomFlip("horizontal_and_vertical"),
    layers.RandomRotation(0.2),
    layers.RandomContrast(0.3)
])

# Directly apply augmentation
aug_train = train_ds_proc.map(lambda x, y: (data_aug(x), y))


# Clone model or reinitialize
aug_model = models.clone_model(model)
aug_model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

aug_history = aug_model.fit(aug_train, validation_data=val_ds_proc, epochs=5)
test_loss2, test_acc2 = aug_model.evaluate(test_ds_proc)
print("Test accuracy after augmentation:", test_acc2)


plt.plot(history.history['accuracy'], label='Train Before Aug')
plt.plot(aug_history.history['accuracy'], label='Train After Aug')
plt.plot(history.history['val_accuracy'], '--', label='Val Before Aug')
plt.plot(aug_history.history['val_accuracy'], '--', label='Val After Aug')
plt.title('Accuracy Comparison')
plt.legend()
plt.show()

print("Test accuracy before:", test_acc)
print("Test accuracy after:", test_acc2)

'''Ex-7-1'''
import cv2
import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage import maximum_filter

img=cv2.imread("1.jpg",cv2.IMREAD_GRAYSCALE)
plt.figure(figsize=(8,8))
plt.imshow(img,cmap='gray')
plt.title("Original Image")
plt.axis("off")
plt.show()

smoothed=cv2.GaussianBlur(img,(5,5),1)
plt.figure(figsize=(8,8))
plt.imshow(smoothed,cmap='gray')
plt.title("Smoothened Image")
plt.axis("off")
plt.show()

gx=cv2.Sobel(smoothed,cv2.CV_64F,1,0,ksize=3)
gy=cv2.Sobel(smoothed,cv2.CV_64F,0,1,ksize=3)
mag=np.sqrt(gx**2+gy**2)
plt.figure(figsize=(8,8))
plt.imshow(mag,cmap='gray')
plt.title("Magnitude")
plt.axis("off")
plt.show()

edge=cv2.Canny(smoothed,100,200)
plt.figure(figsize=(8,8))
plt.imshow(edge,cmap='gray')
plt.title("Edge")
plt.axis("off")
plt.show()

nms=(mag==maximum_filter(mag,size=3))*mag
print("Before NMS",mag/mag.max())
print("After NMS",nms/nms.max())
cv2.waitKey(0)
cv2.destroyAllWindows()

#EX7.2

import cv2
import numpy as np
import matplotlib.pyplot as plt

# 1. Load Image
img = cv2.imread('1.jpg', 0)
if img is None: raise Exception("Image not found!")

# 2. Define Seed and Threshold
seed_point = (100, 100)
threshold = 20

# 3. Apply Region Growing (FloodFill)
# Create a copy so we don't destroy the original image
flood_img = img.copy()

# Mask size must be 2 pixels larger than the image
# This mask will store the actual "shape" of the segmented region
h, w = img.shape
mask = np.zeros((h + 2, w + 2), np.uint8)

# flags=4 (4 neighbors), flags=8 (8 neighbors)
# (255, 255, 255) is the color we paint into 'flood_img'
cv2.floodFill(flood_img, mask, seed_point, 255, loDiff=threshold, upDiff=threshold, flags=4)

# 4. Refine the Mask for Display
# The mask generated by floodFill includes a 1-pixel border we don't need.
# We slice it to match the original image size.
binary_mask = mask[1:-1, 1:-1] * 255  # Scale 0/1 to 0/255 for display

# 5. Display All Three Stages
plt.figure(figsize=(10, 4))
# Plot 1: Original
plt.imshow(img, cmap='gray')
plt.title("1. Original Image")
plt.axis('off')

# Plot 2: The 'Painted' Result (What your code did)
plt.figure(figsize=(10, 4))
plt.imshow(flood_img, cmap='gray')
plt.title("2. Region Highlighted")
plt.axis('off')

# Plot 3: The Actual Segmentation (The Binary Mask)
plt.figure(figsize=(10, 4))
plt.imshow(binary_mask, cmap='gray')
plt.title("3. Final Binary Mask")
plt.axis('off')

plt.show()
